[2024-09-25 11:55:34.937] [info] rag_api_server in src/main.rs:154: server_version: 0.9.4
[2024-09-25 11:55:34.937] [info] rag_api_server in src/main.rs:162: model_name: Qwen1.5-1.8B-Chat-Q5_K_M,nomic-embed-text-v1.5.f16
[2024-09-25 11:55:34.937] [info] rag_api_server in src/main.rs:170: model_alias: default,embedding
[2024-09-25 11:55:34.937] [info] rag_api_server in src/main.rs:184: ctx_size: 4096,8192
[2024-09-25 11:55:34.937] [info] rag_api_server in src/main.rs:198: batch_size: 4096,8192
[2024-09-25 11:55:34.937] [info] rag_api_server in src/main.rs:212: prompt_template: chatml,embedding
[2024-09-25 11:55:34.937] [info] rag_api_server in src/main.rs:220: n_predict: 1024
[2024-09-25 11:55:34.937] [info] rag_api_server in src/main.rs:223: n_gpu_layers: 100
[2024-09-25 11:55:34.937] [info] rag_api_server in src/main.rs:236: threads: 2
[2024-09-25 11:55:34.937] [info] rag_api_server in src/main.rs:250: rag_prompt: The following text is the context for the user question.\n----------------\n
[2024-09-25 11:55:34.937] [info] rag_api_server in src/main.rs:271: qdrant_url: http://127.0.0.1:6333
[2024-09-25 11:55:34.937] [info] rag_api_server in src/main.rs:274: qdrant_collection_name: default
[2024-09-25 11:55:34.937] [info] rag_api_server in src/main.rs:277: qdrant_limit: 3
[2024-09-25 11:55:34.938] [info] rag_api_server in src/main.rs:280: qdrant_score_threshold: 0.5
[2024-09-25 11:55:34.938] [info] rag_api_server in src/main.rs:291: chunk_capacity: 100
[2024-09-25 11:55:34.938] [info] rag_api_server in src/main.rs:294: rag_policy: system-message
[2024-09-25 11:55:34.938] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:434: Initializing the core context for RAG scenarios
[2024-09-25 11:55:34.938] [info] [WASI-NN] GGML backend: LLAMA_COMMIT 8f1d81a0
[2024-09-25 11:55:34.938] [info] [WASI-NN] GGML backend: LLAMA_BUILD_NUMBER 3651
[2024-09-25 11:55:35.010] [info] [WASI-NN] llama.cpp: llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from Qwen1.5-1.8B-Chat-Q5_K_M.gguf (version GGUF V3 (latest))
[2024-09-25 11:55:35.010] [info] [WASI-NN] llama.cpp: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[2024-09-25 11:55:35.010] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
[2024-09-25 11:55:35.010] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   1:                               general.name str              = Qwen1.5-1.8B-Chat
[2024-09-25 11:55:35.010] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   2:                          qwen2.block_count u32              = 24
[2024-09-25 11:55:35.010] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768
[2024-09-25 11:55:35.010] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 2048
[2024-09-25 11:55:35.010] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 5504
[2024-09-25 11:55:35.010] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 16
[2024-09-25 11:55:35.010] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 16
[2024-09-25 11:55:35.010] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   8:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
[2024-09-25 11:55:35.010] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   9:                qwen2.use_parallel_residual bool             = true
[2024-09-25 11:55:35.010] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2
[2024-09-25 11:55:35.027] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
[2024-09-25 11:55:35.038] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
[2024-09-25 11:55:35.055] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
[2024-09-25 11:55:35.055] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 151643
[2024-09-25 11:55:35.055] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  15:            tokenizer.ggml.padding_token_id u32              = 151643
[2024-09-25 11:55:35.055] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 151643
[2024-09-25 11:55:35.055] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  17:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...
[2024-09-25 11:55:35.055] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  18:               general.quantization_version u32              = 2
[2024-09-25 11:55:35.055] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  19:                          general.file_type u32              = 17
[2024-09-25 11:55:35.055] [info] [WASI-NN] llama.cpp: llama_model_loader: - type  f32:  121 tensors
[2024-09-25 11:55:35.055] [info] [WASI-NN] llama.cpp: llama_model_loader: - type q5_1:   12 tensors
[2024-09-25 11:55:35.055] [info] [WASI-NN] llama.cpp: llama_model_loader: - type q8_0:   12 tensors
[2024-09-25 11:55:35.055] [info] [WASI-NN] llama.cpp: llama_model_loader: - type q5_K:  133 tensors
[2024-09-25 11:55:35.055] [info] [WASI-NN] llama.cpp: llama_model_loader: - type q6_K:   13 tensors
[2024-09-25 11:55:35.142] [warning] [WASI-NN] llama.cpp: llm_load_vocab: missing pre-tokenizer type, using: 'default'
[2024-09-25 11:55:35.142] [warning] [WASI-NN] llama.cpp: llm_load_vocab:                                             
[2024-09-25 11:55:35.142] [warning] [WASI-NN] llama.cpp: llm_load_vocab: ************************************        
[2024-09-25 11:55:35.142] [warning] [WASI-NN] llama.cpp: llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        
[2024-09-25 11:55:35.142] [warning] [WASI-NN] llama.cpp: llm_load_vocab: CONSIDER REGENERATING THE MODEL             
[2024-09-25 11:55:35.142] [warning] [WASI-NN] llama.cpp: llm_load_vocab: ************************************        
[2024-09-25 11:55:35.142] [warning] [WASI-NN] llama.cpp: llm_load_vocab:                                             
[2024-09-25 11:55:35.183] [info] [WASI-NN] llama.cpp: llm_load_vocab: special tokens cache size = 293
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_vocab: token to piece cache size = 0.9338 MB
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: format           = GGUF V3 (latest)
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: arch             = qwen2
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: vocab type       = BPE
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_vocab          = 151936
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_merges         = 151387
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: vocab_only       = 0
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ctx_train      = 32768
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd           = 2048
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_layer          = 24
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_head           = 16
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_head_kv        = 16
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_rot            = 128
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_swa            = 0
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_head_k    = 128
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_head_v    = 128
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_gqa            = 1
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_k_gqa     = 2048
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_v_gqa     = 2048
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_norm_eps       = 0.0e+00
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_logit_scale    = 0.0e+00
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ff             = 5504
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_expert         = 0
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_expert_used    = 0
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: causal attn      = 1
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: pooling type     = 0
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope type        = 2
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope scaling     = linear
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: freq_base_train  = 10000.0
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: freq_scale_train = 1
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ctx_orig_yarn  = 32768
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope_finetuned   = unknown
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_conv       = 0
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_inner      = 0
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_state      = 0
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_dt_rank      = 0
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_dt_b_c_rms   = 0
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model type       = 1B
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model ftype      = Q5_K - Medium
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model params     = 1.84 B
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model size       = 1.28 GiB (5.97 BPW) 
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: general.name     = Qwen1.5-1.8B-Chat
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: EOS token        = 151643 '<|endoftext|>'
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: LF token         = 148848 'ÄĬ'
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: EOT token        = 151645 '<|im_end|>'
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_print_meta: max token length = 256
[2024-09-25 11:55:35.227] [info] [WASI-NN] llama.cpp: llm_load_tensors: ggml ctx size =    0.13 MiB
[2024-09-25 11:55:35.275] [info] [WASI-NN] llama.cpp: llm_load_tensors:        CPU buffer size =  1307.33 MiB
[2024-09-25 11:55:35.275] [info] [WASI-NN] llama.cpp: 
[2024-09-25 11:55:35.281] [info] [WASI-NN] GGML backend: llama_system_info: AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
[2024-09-25 11:55:35.281] [info] [WASI-NN] GGML backend: LLAMA_COMMIT 8f1d81a0
[2024-09-25 11:55:35.281] [info] [WASI-NN] GGML backend: LLAMA_BUILD_NUMBER 3651
[2024-09-25 11:55:35.286] [info] [WASI-NN] llama.cpp: llama_model_loader: loaded meta data with 22 key-value pairs and 112 tensors from nomic-embed-text-v1.5.f16.gguf (version GGUF V3 (latest))
[2024-09-25 11:55:35.286] [info] [WASI-NN] llama.cpp: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[2024-09-25 11:55:35.286] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
[2024-09-25 11:55:35.286] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
[2024-09-25 11:55:35.286] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
[2024-09-25 11:55:35.286] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
[2024-09-25 11:55:35.286] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
[2024-09-25 11:55:35.286] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
[2024-09-25 11:55:35.286] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
[2024-09-25 11:55:35.286] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
[2024-09-25 11:55:35.286] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   8:                          general.file_type u32              = 1
[2024-09-25 11:55:35.286] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
[2024-09-25 11:55:35.286] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
[2024-09-25 11:55:35.286] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
[2024-09-25 11:55:35.286] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
[2024-09-25 11:55:35.286] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
[2024-09-25 11:55:35.286] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
[2024-09-25 11:55:35.286] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
[2024-09-25 11:55:35.289] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
[2024-09-25 11:55:35.297] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
[2024-09-25 11:55:35.299] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
[2024-09-25 11:55:35.299] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
[2024-09-25 11:55:35.299] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
[2024-09-25 11:55:35.299] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
[2024-09-25 11:55:35.299] [info] [WASI-NN] llama.cpp: llama_model_loader: - type  f32:   51 tensors
[2024-09-25 11:55:35.299] [info] [WASI-NN] llama.cpp: llama_model_loader: - type  f16:   61 tensors
[2024-09-25 11:55:35.305] [info] [WASI-NN] llama.cpp: llm_load_vocab: special tokens cache size = 5
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_vocab: token to piece cache size = 0.2032 MB
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: format           = GGUF V3 (latest)
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: arch             = nomic-bert
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: vocab type       = WPM
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_vocab          = 30522
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_merges         = 0
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: vocab_only       = 0
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ctx_train      = 2048
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd           = 768
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_layer          = 12
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_head           = 12
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_head_kv        = 12
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_rot            = 64
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_swa            = 0
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_head_k    = 64
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_head_v    = 64
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_gqa            = 1
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_k_gqa     = 768
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_v_gqa     = 768
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_norm_eps       = 1.0e-12
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_logit_scale    = 0.0e+00
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ff             = 3072
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_expert         = 0
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_expert_used    = 0
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: causal attn      = 0
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: pooling type     = 1
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope type        = 2
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope scaling     = linear
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: freq_base_train  = 1000.0
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: freq_scale_train = 1
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ctx_orig_yarn  = 2048
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope_finetuned   = unknown
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_conv       = 0
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_inner      = 0
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_state      = 0
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_dt_rank      = 0
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_dt_b_c_rms   = 0
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model type       = 137M
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model ftype      = F16
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model params     = 136.73 M
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: general.name     = nomic-embed-text-v1.5
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: BOS token        = 101 '[CLS]'
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: EOS token        = 102 '[SEP]'
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: UNK token        = 100 '[UNK]'
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: SEP token        = 102 '[SEP]'
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: PAD token        = 0 '[PAD]'
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: CLS token        = 101 '[CLS]'
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: MASK token       = 103 '[MASK]'
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: LF token         = 0 '[PAD]'
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_print_meta: max token length = 21
[2024-09-25 11:55:35.308] [info] [WASI-NN] llama.cpp: llm_load_tensors: ggml ctx size =    0.05 MiB
[2024-09-25 11:55:35.316] [info] [WASI-NN] llama.cpp: llm_load_tensors:        CPU buffer size =   260.86 MiB
[2024-09-25 11:55:35.316] [info] [WASI-NN] llama.cpp: 
[2024-09-25 11:55:35.317] [info] [WASI-NN] GGML backend: llama_system_info: AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
[2024-09-25 11:55:35.317] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:489: running mode: rag
[2024-09-25 11:55:35.317] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:502: The core context for RAG scenarios has been initialized
[2024-09-25 11:55:35.317] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:512: Getting the plugin info
[2024-09-25 11:55:35.317] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 11:55:35.317] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 11:55:35.317] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:592: Getting the plugin info by the graph named Qwen1.5-1.8B-Chat-Q5_K_M
[2024-09-25 11:55:35.317] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M
[2024-09-25 11:55:35.317] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-25 11:55:35.317] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:652: Plugin info: b3651(commit 8f1d81a0)
[2024-09-25 11:55:35.317] [info] rag_api_server in src/main.rs:404: plugin_ggml_version: b3651 (commit 8f1d81a0)
[2024-09-25 11:55:35.317] [info] rag_api_server in src/main.rs:414: socket_address: 0.0.0.0:8080
[2024-09-25 11:55:35.317] [info] rag_api_server in src/main.rs:421: gaianet_node_version: 0.4.3
[2024-09-25 11:55:46.756] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:54418, local_addr: 0.0.0.0:8080
[2024-09-25 11:55:46.756] [info] rag_api_server in src/main.rs:495: method: POST, http_version: HTTP/1.1, content-length: 99
[2024-09-25 11:55:46.756] [info] rag_api_server in src/main.rs:496: endpoint: /v1/chat/completions
[2024-09-25 11:55:46.756] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-25 11:55:46.756] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-25 11:55:46.757] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-2a7ab2ec-3155-4356-9a1f-97c79b3ecc45
[2024-09-25 11:55:46.757] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-25 11:55:46.757] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: What is your name?
[2024-09-25 11:55:46.757] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-25 11:55:46.757] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-25 11:55:46.757] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 11:55:46.757] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 11:55:46.757] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-25 11:55:46.757] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 11:55:46.757] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 11:55:46.757] [info] llama_core::graph in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/graph.rs:296: Update metadata for the model named nomic-embed-text-v1.5.f16
[2024-09-25 11:55:46.757] [info] llama_core::graph in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/graph.rs:314: Metadata updated successfully.
[2024-09-25 11:55:46.757] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-25 11:55:46.757] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 8192
[2024-09-25 11:55:46.757] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-25 11:55:46.757] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-25 11:55:46.757] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 11:55:46.757] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-25 11:55:46.757] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 11:55:46.800] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
[2024-09-25 11:55:46.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
[2024-09-25 11:55:46.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-25 11:55:46.802] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-25 11:55:46.802] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-25 11:55:46.802] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 11:55:46.802] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-25 11:55:46.802] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 8192
[2024-09-25 11:55:46.802] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-25 11:55:46.802] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-25 11:55:46.802] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 11:55:46.802] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-25 11:55:46.802] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 11:55:46.822] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
[2024-09-25 11:55:46.822] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
[2024-09-25 11:55:46.822] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-25 11:55:46.823] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-25 11:55:46.823] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-25 11:55:46.823] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 11:55:46.857] [info] [WASI-NN] llama.cpp: 
[2024-09-25 11:55:46.857] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =   11549.41 ms
[2024-09-25 11:55:46.857] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-25 11:55:46.857] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      24.81 ms /     7 tokens (    3.54 ms per token,   282.16 tokens per second)
[2024-09-25 11:55:46.857] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-25 11:55:46.857] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =   11548.97 ms /     8 tokens
[2024-09-25 11:55:46.858] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named nomic-embed-text-v1.5.f16
[2024-09-25 11:55:46.858] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11281
[2024-09-25 11:55:46.863] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named nomic-embed-text-v1.5.f16.
[2024-09-25 11:55:46.863] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named nomic-embed-text-v1.5.f16
[2024-09-25 11:55:46.863] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-25 11:55:46.863] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 7, completion tokens: 0
[2024-09-25 11:55:46.863] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 7 prompt tokens, 0 comletion tokens
[2024-09-25 11:55:46.863] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-25 11:55:46.863] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-25 11:55:46.863] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 3, score_threshold: 0.5
[2024-09-25 11:55:46.863] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 11:55:46.863] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 11:55:46.863] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-25 11:55:46.880] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-25 11:55:46.881] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-25 11:55:46.881] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-25 11:55:46.881] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-25 11:55:46.881] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-25 11:55:46.881] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-25 11:55:46.881] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 11:55:46.881] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 11:55:46.881] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-2a7ab2ec-3155-4356-9a1f-97c79b3ecc45
[2024-09-25 11:55:46.881] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-25 11:55:46.881] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-25 11:55:46.881] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-25 11:55:46.881] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-25 11:55:46.881] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 11:55:46.881] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-25 11:55:46.881] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 4096
[2024-09-25 11:55:46.881] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 4096
[2024-09-25 11:55:46.881] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 11:55:46.881] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-25 11:55:46.881] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 11:55:46.975] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB
[2024-09-25 11:55:46.975] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
[2024-09-25 11:55:46.975] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB
[2024-09-25 11:55:46.976] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =  2406.00 MiB
[2024-09-25 11:55:46.976] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 846
[2024-09-25 11:55:46.976] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 11:55:46.977] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 11:55:46.977] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M
[2024-09-25 11:55:46.977] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 96
[2024-09-25 11:55:46.978] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 25, completion tokens: 0
[2024-09-25 11:55:46.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|im_start|>system
Answer as concisely as possible.<|im_end|>
<|im_start|>user
What is your name?<|im_end|>
<|im_start|>assistant
[2024-09-25 11:55:46.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-25 11:55:46.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-25 11:55:46.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-25 11:55:46.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-25 11:55:46.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-25 11:55:46.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 11:55:46.978] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-25 11:55:46.978] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 4096
[2024-09-25 11:55:46.978] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 4096
[2024-09-25 11:55:46.978] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 11:55:46.978] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-25 11:55:46.978] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 11:55:47.031] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB
[2024-09-25 11:55:47.031] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
[2024-09-25 11:55:47.031] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB
[2024-09-25 11:55:47.032] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =  2406.00 MiB
[2024-09-25 11:55:47.032] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 846
[2024-09-25 11:55:47.032] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 11:55:47.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-25 11:55:47.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 11:55:47.034] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-25 11:55:47.034] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 4096
[2024-09-25 11:55:47.034] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 4096
[2024-09-25 11:55:47.034] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 11:55:47.034] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-25 11:55:47.034] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 11:55:47.087] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB
[2024-09-25 11:55:47.087] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
[2024-09-25 11:55:47.087] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB
[2024-09-25 11:55:47.088] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =  2406.00 MiB
[2024-09-25 11:55:47.088] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 846
[2024-09-25 11:55:47.088] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 11:55:50.550] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-25 11:55:50.550] [info] [WASI-NN] llama.cpp: 
[2024-09-25 11:55:50.550] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =   12716.47 ms
[2024-09-25 11:55:50.550] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =      65.82 ms /    40 runs   (    1.65 ms per token,   607.71 tokens per second)
[2024-09-25 11:55:50.550] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =     855.17 ms /    25 tokens (   34.21 ms per token,    29.23 tokens per second)
[2024-09-25 11:55:50.550] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    2508.77 ms /    39 runs   (   64.33 ms per token,    15.55 tokens per second)
[2024-09-25 11:55:50.550] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =   15322.94 ms /    64 tokens
[2024-09-25 11:55:50.551] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M
[2024-09-25 11:55:50.551] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 188
[2024-09-25 11:55:50.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation: As an AI language model, I don't have a name in the traditional sense. My "name" is simply the programming name or label assigned to me by my developers or training data sources.<|im_end|>
[2024-09-25 11:55:50.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-25 11:55:50.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
As an AI language model, I don't have a name in the traditional sense. My "name" is simply the programming name or label assigned to me by my developers or training data sources.
[2024-09-25 11:55:50.552] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 11:55:50.552] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M
[2024-09-25 11:55:50.552] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-25 11:55:50.552] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 25, completion tokens: 40
[2024-09-25 11:55:50.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 25, completion tokens: 40
[2024-09-25 11:55:50.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-25 11:55:50.552] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-25 11:55:50.552] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-25 11:55:50.552] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-25 11:55:50.552] [info] rag_api_server in src/main.rs:517: response_body_size: 494
[2024-09-25 11:55:50.552] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-25 11:55:50.552] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-25 11:56:21.930] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:48456, local_addr: 0.0.0.0:8080
[2024-09-25 11:56:21.932] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-25 11:56:21.932] [info] rag_api_server in src/main.rs:499: endpoint: /v1/info
[2024-09-25 11:56:21.932] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:1677: Handling the coming server info request.
[2024-09-25 11:56:21.933] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:1724: Send the server info response.
[2024-09-25 11:56:21.933] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-25 11:56:21.933] [info] rag_api_server in src/main.rs:517: response_body_size: 810
[2024-09-25 11:56:21.933] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-25 11:56:21.933] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-25 11:56:31.547] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-25 11:56:31.548] [info] rag_api_server in src/main.rs:499: endpoint: /config_pub.json
[2024-09-25 11:56:31.551] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-25 11:56:31.551] [info] rag_api_server in src/main.rs:517: response_body_size: 1084
[2024-09-25 11:56:31.551] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-25 11:56:31.551] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-25 11:59:17.221] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:44578, local_addr: 0.0.0.0:8080
[2024-09-25 11:59:17.222] [info] rag_api_server in src/main.rs:498: method: OPTIONS, http_version: HTTP/1.1
[2024-09-25 11:59:17.222] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-25 11:59:17.222] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-25 11:59:17.222] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-25 11:59:17.222] [info] rag_api_server in src/main.rs:517: response_body_size: 0
[2024-09-25 11:59:17.222] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-25 11:59:17.222] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-25 11:59:18.182] [info] rag_api_server in src/main.rs:495: method: POST, http_version: HTTP/1.1, content-length: 270
[2024-09-25 11:59:18.182] [info] rag_api_server in src/main.rs:496: endpoint: /v1/chat/completions
[2024-09-25 11:59:18.182] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-25 11:59:18.182] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-25 11:59:18.182] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: 9d3905ee-007f-4959-a3a1-d857206efd11
[2024-09-25 11:59:18.182] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-25 11:59:18.182] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: what is a Transmitter
[2024-09-25 11:59:18.182] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-25 11:59:18.182] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-25 11:59:18.182] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 11:59:18.182] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 11:59:18.182] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-25 11:59:18.182] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 11:59:18.183] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 11:59:18.183] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-25 11:59:18.183] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 8192
[2024-09-25 11:59:18.183] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-25 11:59:18.183] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-25 11:59:18.183] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 11:59:18.183] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-25 11:59:18.183] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 11:59:18.226] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
[2024-09-25 11:59:18.226] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
[2024-09-25 11:59:18.226] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-25 11:59:18.227] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-25 11:59:18.227] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-25 11:59:18.227] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 11:59:18.227] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-25 11:59:18.227] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 8192
[2024-09-25 11:59:18.227] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-25 11:59:18.227] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-25 11:59:18.227] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 11:59:18.227] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-25 11:59:18.227] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 11:59:18.249] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
[2024-09-25 11:59:18.249] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
[2024-09-25 11:59:18.249] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-25 11:59:18.249] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-25 11:59:18.249] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-25 11:59:18.249] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 11:59:18.280] [info] [WASI-NN] llama.cpp: 
[2024-09-25 11:59:18.280] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  222971.69 ms
[2024-09-25 11:59:18.280] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-25 11:59:18.280] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      19.69 ms /     6 tokens (    3.28 ms per token,   304.69 tokens per second)
[2024-09-25 11:59:18.280] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-25 11:59:18.280] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  222970.97 ms /     7 tokens
[2024-09-25 11:59:18.280] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named nomic-embed-text-v1.5.f16
[2024-09-25 11:59:18.280] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11251
[2024-09-25 11:59:18.285] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named nomic-embed-text-v1.5.f16.
[2024-09-25 11:59:18.285] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named nomic-embed-text-v1.5.f16
[2024-09-25 11:59:18.285] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-25 11:59:18.286] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-25 11:59:18.286] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-25 11:59:18.286] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-25 11:59:18.286] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-25 11:59:18.286] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 3, score_threshold: 0.5
[2024-09-25 11:59:18.286] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 11:59:18.286] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 11:59:18.286] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-25 11:59:18.355] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 3
[2024-09-25 11:59:18.355] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:388: point: 0, score: 0.65887666, source: "Transmitter and Receiver: The transmitter sends signals to the receiver on the aircraft, which controls the servos attached to control surfaces.\nServos: Small motors that move the control surfaces to execute commands from the transmitter.\nBattery Systems: Electric models use lightweight lithium polymer (LiPo) batteries for power.\nSection 7: Aeromodelling in Practice: Common Challenges\n"
[2024-09-25 11:59:18.355] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:388: point: 1, score: 0.6360044, source: "Radio Control System: This system allows the pilot to remotely control the model, typically using a hand-held transmitter and a receiver in the aircraft.\n"
[2024-09-25 11:59:18.355] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:388: point: 2, score: 0.5593585, source: "Section 6: Radio Control (RC) and Electronics\n"
[2024-09-25 11:59:18.355] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:87: Get the chat prompt template type from the chat model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 11:59:18.355] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:122: prompt_template: chatml
[2024-09-25 11:59:18.356] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:567: rag_policy: system-message
[2024-09-25 11:59:18.356] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:571: context:
"Transmitter and Receiver: The transmitter sends signals to the receiver on the aircraft, which controls the servos attached to control surfaces.\nServos: Small motors that move the control surfaces to execute commands from the transmitter.\nBattery Systems: Electric models use lightweight lithium polymer (LiPo) batteries for power.\nSection 7: Aeromodelling in Practice: Common Challenges\n"

"Radio Control System: This system allows the pilot to remotely control the model, typically using a hand-held transmitter and a receiver in the aircraft.\n"

"Section 6: Radio Control (RC) and Electronics\n"
[2024-09-25 11:59:18.356] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:575: Merge RAG context into system message.
[2024-09-25 11:59:18.356] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-25 11:59:18.356] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-25 11:59:18.356] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(true)
[2024-09-25 11:59:18.356] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:82: Process chat completion request in the stream mode.
[2024-09-25 11:59:18.356] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 11:59:18.356] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 11:59:18.356] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:104: user: 9d3905ee-007f-4959-a3a1-d857206efd11
[2024-09-25 11:59:18.356] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:113: include_usage: true
[2024-09-25 11:59:18.356] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-25 11:59:18.356] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-25 11:59:18.356] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-25 11:59:18.356] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-25 11:59:18.356] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 11:59:18.356] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-25 11:59:18.356] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 4096
[2024-09-25 11:59:18.356] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 4096
[2024-09-25 11:59:18.356] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 11:59:18.356] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-25 11:59:18.356] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 11:59:18.450] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB
[2024-09-25 11:59:18.450] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
[2024-09-25 11:59:18.450] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB
[2024-09-25 11:59:18.450] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =  2406.00 MiB
[2024-09-25 11:59:18.450] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 846
[2024-09-25 11:59:18.451] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 11:59:18.453] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 11:59:18.453] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M
[2024-09-25 11:59:18.453] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 98
[2024-09-25 11:59:18.453] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 176, completion tokens: 40
[2024-09-25 11:59:18.453] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:124: prompt:
<|im_start|>system
You are a Aeromodelling expert in India.
The following text is the context for the user question.\n----------------\n
"Transmitter and Receiver: The transmitter sends signals to the receiver on the aircraft, which controls the servos attached to control surfaces.\nServos: Small motors that move the control surfaces to execute commands from the transmitter.\nBattery Systems: Electric models use lightweight lithium polymer (LiPo) batteries for power.\nSection 7: Aeromodelling in Practice: Common Challenges\n"

"Radio Control System: This system allows the pilot to remotely control the model, typically using a hand-held transmitter and a receiver in the aircraft.\n"

"Section 6: Radio Control (RC) and Electronics\n"<|im_end|>
<|im_start|>user
what is a Transmitter<|im_end|>
<|im_start|>assistant
[2024-09-25 11:59:18.453] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:125: available_completion_tokens: 820
[2024-09-25 11:59:18.453] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:126: tool_use: false
[2024-09-25 11:59:18.453] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-25 11:59:18.453] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-25 11:59:18.453] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-25 11:59:18.454] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 11:59:18.454] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-25 11:59:18.454] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 4096
[2024-09-25 11:59:18.454] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 4096
[2024-09-25 11:59:18.454] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 11:59:18.454] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-25 11:59:18.454] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 11:59:18.508] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB
[2024-09-25 11:59:18.508] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
[2024-09-25 11:59:18.508] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB
[2024-09-25 11:59:18.508] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =  2406.00 MiB
[2024-09-25 11:59:18.508] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 846
[2024-09-25 11:59:18.508] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 11:59:18.511] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:193: End of the chat completion stream.
[2024-09-25 11:59:18.511] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:461: finish chat completions in stream mode
[2024-09-25 11:59:18.511] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-25 11:59:18.511] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-25 11:59:18.511] [info] rag_api_server in src/main.rs:517: response_body_size: 0
[2024-09-25 11:59:18.511] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-25 11:59:18.511] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-25 11:59:18.512] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:18.512] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-25 11:59:18.512] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 4096
[2024-09-25 11:59:18.512] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 4096
[2024-09-25 11:59:18.512] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 11:59:18.512] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-25 11:59:18.512] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 11:59:18.566] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB
[2024-09-25 11:59:18.566] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
[2024-09-25 11:59:18.566] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB
[2024-09-25 11:59:18.567] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =  2406.00 MiB
[2024-09-25 11:59:18.567] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 846
[2024-09-25 11:59:18.567] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 11:59:24.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:24.856] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:24.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:24.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:24.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:24.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:24.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":"A","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258364,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:24.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:24.926] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:24.926] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:24.926] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:24.926] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:24.926] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:24.926] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:24.926] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" transmitter","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258364,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:24.926] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:24.996] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:24.996] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:24.997] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:24.997] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:24.997] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:24.997] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:24.997] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" is","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258364,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:24.997] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:25.067] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:25.067] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:25.067] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:25.067] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:25.068] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:25.068] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:25.068] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" a","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258365,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:25.068] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:25.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:25.137] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:25.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:25.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:25.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:25.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:25.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" device","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258365,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:25.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:25.209] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:25.209] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:25.209] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:25.209] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:25.209] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:25.210] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:25.210] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" used","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258365,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:25.210] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:25.287] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:25.287] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:25.287] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:25.287] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:25.287] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:25.287] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:25.287] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" to","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258365,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:25.287] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:25.357] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:25.357] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:25.357] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:25.357] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:25.357] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:25.357] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:25.357] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" transmit","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258365,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:25.357] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:25.427] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:25.427] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:25.427] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:25.427] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:25.427] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:25.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:25.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" signals","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258365,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:25.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:25.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:25.500] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:25.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:25.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:25.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:25.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:25.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258365,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:25.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:25.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:25.582] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:25.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:25.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:25.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:25.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:25.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" such","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258365,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:25.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:25.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:25.653] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:25.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:25.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:25.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:25.654] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:25.654] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" as","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258365,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:25.654] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:25.726] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:25.726] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:25.726] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:25.726] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:25.726] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:25.726] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:25.727] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" radio","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258365,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:25.727] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:25.799] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:25.799] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:25.799] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:25.799] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:25.799] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:25.799] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:25.799] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" or","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258365,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:25.799] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:25.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:25.870] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:25.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:25.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:25.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:25.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:25.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" television","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258365,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:25.871] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:25.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:25.940] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:25.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:25.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:25.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:25.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:25.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" signals","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258365,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:25.943] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:26.016] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:26.016] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:26.016] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:26.016] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:26.016] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:26.016] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:26.016] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258366,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:26.016] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:26.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:26.086] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:26.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:26.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:26.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:26.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:26.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" from","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258366,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:26.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:26.162] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:26.162] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:26.162] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:26.162] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:26.162] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:26.162] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:26.162] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" one","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258366,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:26.162] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:26.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:26.236] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:26.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:26.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:26.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:26.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:26.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" location","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258366,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:26.237] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:26.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:26.315] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:26.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:26.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:26.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:26.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:26.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" or","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258366,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:26.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:26.391] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:26.391] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:26.391] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:26.391] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:26.391] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:26.391] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:26.391] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" point","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258366,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:26.391] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:26.467] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:26.467] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:26.467] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:26.467] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:26.467] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:26.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:26.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" on","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258366,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:26.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:26.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:26.540] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:26.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:26.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:26.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:26.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:26.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" an","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258366,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:26.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:26.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:26.614] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:26.614] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:26.614] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:26.614] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:26.614] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:26.614] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" aircraft","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258366,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:26.614] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:26.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:26.686] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:26.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:26.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:26.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:26.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:26.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258366,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:26.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:26.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:26.762] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:26.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:26.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:26.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:26.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:26.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" a","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258366,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:26.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:26.832] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:26.832] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:26.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:26.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:26.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:26.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:26.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" vehicle","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258366,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:26.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:26.903] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:26.903] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:26.903] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:26.903] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:26.903] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:26.903] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:26.903] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258366,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:26.903] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:26.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:26.984] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:26.984] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:26.984] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:26.984] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:26.984] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:26.984] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" or","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258366,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:26.984] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:27.056] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:27.056] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:27.056] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:27.056] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:27.056] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:27.056] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:27.056] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" a","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258367,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:27.056] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:27.129] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:27.129] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:27.129] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:27.129] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:27.129] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:27.129] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:27.129] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" similar","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258367,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:27.133] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:27.206] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:27.206] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:27.206] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:27.206] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:27.206] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:27.206] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:27.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" mechanical","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258367,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:27.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:27.281] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:27.281] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:27.281] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:27.281] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:27.281] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:27.281] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:27.281] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" system","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258367,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:27.281] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:27.353] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:27.353] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:27.353] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:27.353] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:27.353] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:27.353] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:27.353] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":".\n\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258367,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:27.353] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:27.431] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:27.431] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:27.431] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:27.431] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:27.431] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:27.432] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:27.432] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":"In","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258367,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:27.432] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:27.504] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:27.504] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:27.504] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:27.504] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:27.504] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:27.505] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:27.505] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" the","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258367,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:27.505] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:27.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:27.576] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:27.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:27.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:27.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:27.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:27.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" context","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258367,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:27.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:27.649] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:27.649] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:27.649] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:27.649] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:27.649] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:27.650] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:27.650] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" of","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258367,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:27.650] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:27.720] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:27.720] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:27.720] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:27.720] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:27.720] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:27.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:27.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" aer","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258367,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:27.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:27.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:27.792] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:27.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:27.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:27.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:27.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:27.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":"om","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258367,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:27.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:27.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:27.863] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:27.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:27.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:27.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:27.864] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:27.864] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":"od","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258367,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:27.864] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:27.935] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:27.935] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:27.935] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:27.935] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:27.935] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:27.935] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:27.935] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":"elling","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258367,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:27.935] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:28.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:28.011] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:28.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:28.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:28.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:28.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:28.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258368,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:28.012] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:28.094] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:28.094] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:28.094] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:28.094] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:28.094] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:28.094] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:28.094] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" a","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258368,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:28.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:28.172] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:28.172] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:28.172] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:28.172] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:28.172] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:28.172] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:28.172] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" transmitter","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258368,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:28.172] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:28.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:28.248] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:28.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:28.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:28.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:28.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:28.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" is","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258368,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:28.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:28.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:28.331] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:28.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:28.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:28.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:28.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:28.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" typically","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258368,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:28.333] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:28.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:28.408] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:28.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:28.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:28.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:28.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:28.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" responsible","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258368,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:28.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:28.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:28.484] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:28.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:28.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:28.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:28.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:28.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" for","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258368,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:28.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:28.556] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:28.556] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:28.556] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:28.556] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:28.556] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:28.556] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:28.556] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" transmitting","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258368,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:28.556] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:28.632] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:28.632] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:28.632] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:28.632] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:28.632] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:28.632] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:28.632] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" signals","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258368,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:28.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:28.704] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:28.704] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:28.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:28.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:28.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:28.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:28.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" over","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258368,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:28.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:28.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:28.775] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:28.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:28.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:28.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:28.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:28.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" long","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258368,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:28.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:28.849] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:28.849] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:28.849] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:28.849] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:28.849] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:28.849] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:28.849] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" distances","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258368,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:28.849] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:28.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:28.920] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:28.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:28.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:28.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:28.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:28.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258368,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:28.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:28.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:28.990] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:28.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:28.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:28.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:28.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:28.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" often","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258368,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:28.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:29.083] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:29.083] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:29.083] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:29.083] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:29.083] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:29.083] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:29.083] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" through","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258369,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:29.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:29.156] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:29.156] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:29.156] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:29.156] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:29.156] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:29.156] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:29.156] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" radio","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258369,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:29.156] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:29.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:29.239] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:29.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:29.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:29.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:29.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:29.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" waves","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258369,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:29.240] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:29.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:29.358] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:29.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:29.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:29.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:29.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:29.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":".","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258369,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:29.359] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:29.431] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:29.431] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:29.431] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:29.431] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:29.431] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:29.431] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:29.431] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" This","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258369,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:29.431] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:29.503] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:29.503] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:29.503] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:29.503] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:29.503] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:29.504] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:29.504] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" may","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258369,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:29.504] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:29.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:29.574] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:29.575] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:29.575] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:29.575] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:29.575] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:29.575] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" involve","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258369,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:29.575] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:29.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:29.653] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:29.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:29.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:29.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:29.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:29.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" adjusting","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258369,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:29.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:29.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:29.723] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:29.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:29.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:29.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:29.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:29.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" the","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258369,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:29.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:29.799] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:29.799] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:29.799] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:29.799] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:29.799] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:29.800] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:29.800] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" frequency","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258369,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:29.800] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:29.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:29.870] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:29.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:29.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:29.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:29.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:29.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258369,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:29.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:29.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:29.948] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:29.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:29.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:29.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:29.949] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:29.949] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" power","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258369,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:29.949] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:30.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:30.022] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:30.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:30.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:30.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:30.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:30.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" of","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258370,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:30.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:30.092] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:30.093] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:30.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:30.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:30.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:30.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:30.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" the","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258370,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:30.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:30.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:30.175] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:30.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:30.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:30.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:30.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:30.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" transmitted","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258370,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:30.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:30.247] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:30.247] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:30.247] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:30.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:30.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:30.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:30.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" signal","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258370,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:30.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:30.319] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:30.319] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:30.319] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:30.319] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:30.319] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:30.319] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:30.319] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" to","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258370,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:30.321] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:30.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:30.400] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:30.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:30.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:30.401] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:30.401] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:30.401] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" optimize","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258370,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:30.401] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:30.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:30.473] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:30.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:30.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:30.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:30.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:30.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" its","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258370,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:30.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:30.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:30.544] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:30.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:30.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:30.545] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:30.545] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:30.545] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" performance","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258370,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:30.545] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:30.615] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:30.615] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:30.615] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:30.615] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:30.615] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:30.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:30.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" in","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258370,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:30.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:30.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:30.687] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:30.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:30.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:30.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:30.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:30.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" different","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258370,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:30.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:30.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:30.758] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:30.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:30.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:30.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:30.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:30.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" atmospheric","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258370,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:30.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:30.832] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:30.833] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:30.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:30.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:30.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:30.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:30.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" conditions","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258370,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:30.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:30.903] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:30.903] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:30.903] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:30.903] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:30.903] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:30.903] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:30.903] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":".\n\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258370,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:30.903] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:30.975] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:30.975] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:30.975] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:30.975] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:30.975] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:30.975] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:30.975] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":"Trans","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258370,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:30.975] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:31.048] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:31.048] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:31.048] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:31.048] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:31.048] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:31.048] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:31.048] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":"mitters","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258371,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:31.048] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:31.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:31.121] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:31.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:31.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:31.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:31.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:31.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" are","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258371,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:31.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:31.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:31.194] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:31.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:31.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:31.195] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:31.195] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:31.195] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" essential","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258371,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:31.195] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:31.266] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:31.266] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:31.266] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:31.266] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:31.266] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:31.266] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:31.266] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" components","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258371,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:31.266] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:31.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:31.337] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:31.338] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:31.338] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:31.338] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:31.338] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:31.338] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" of","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258371,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:31.338] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:31.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:31.409] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:31.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:31.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:31.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:31.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:31.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" a","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258371,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:31.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:31.483] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:31.483] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:31.483] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:31.483] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:31.483] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:31.483] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:31.483] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" wide","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258371,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:31.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:31.561] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:31.561] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:31.562] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:31.562] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:31.562] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:31.562] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:31.562] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" range","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258371,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:31.562] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:31.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:31.636] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:31.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:31.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:31.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:31.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:31.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" of","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258371,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:31.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:31.706] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:31.706] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:31.706] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:31.706] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:31.706] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:31.707] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:31.707] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" aircraft","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258371,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:31.707] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:31.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:31.781] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:31.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:31.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:31.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:31.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:31.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258371,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:31.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:31.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:31.852] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:31.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:31.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:31.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:31.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:31.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" vehicles","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258371,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:31.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:31.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:31.924] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:31.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:31.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:31.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:31.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:31.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258371,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:31.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:31.996] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:31.996] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:31.996] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:31.997] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:31.997] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:31.997] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:31.997] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258371,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:31.997] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:32.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:32.070] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:32.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:32.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:32.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:32.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:32.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" mechanical","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258372,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:32.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:32.142] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:32.142] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:32.142] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:32.142] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:32.142] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:32.142] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:32.142] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" systems","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258372,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:32.142] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:32.214] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:32.214] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:32.214] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:32.214] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:32.215] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:32.215] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:32.215] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" used","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258372,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:32.215] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:32.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:32.298] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:32.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:32.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:32.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:32.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:32.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" in","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258372,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:32.301] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:32.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:32.374] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:32.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:32.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:32.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:32.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:32.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" modern","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258372,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:32.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:32.447] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:32.448] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:32.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:32.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:32.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:32.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:32.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" aviation","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258372,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:32.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:32.519] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:32.519] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:32.519] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:32.519] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:32.519] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:32.519] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:32.519] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258372,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:32.519] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:32.591] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:32.591] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:32.591] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:32.591] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:32.591] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:32.591] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:32.591] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" other","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258372,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:32.591] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:32.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:32.666] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:32.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:32.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:32.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:32.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:32.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":" fields","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258372,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:32.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:32.740] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 11:59:32.740] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 11:59:32.740] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 11:59:32.740] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 11:59:32.740] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 11:59:32.740] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:32.740] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[{"index":0,"delta":{"content":".","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727258372,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 11:59:32.740] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:32.743] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-25 11:59:32.743] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 11:59:32.743] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M
[2024-09-25 11:59:32.743] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 99
[2024-09-25 11:59:32.743] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 176, completion tokens: 108
[2024-09-25 11:59:32.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2845: token_info: 176 prompt tokens, 108 completion tokens
[2024-09-25 11:59:32.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:32.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"9d3905ee-007f-4959-a3a1-d857206efd11","choices":[],"created":1727258372,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk","usage":{"prompt_tokens":176,"completion_tokens":108,"total_tokens":284}}


[2024-09-25 11:59:32.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:32.747] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-25 11:59:32.747] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 11:59:32.747] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: [DONE]


[2024-09-25 11:59:32.747] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 11:59:32.747] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2682: Return the chat stream chunk!
[2024-09-25 11:59:32.747] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: [GGML] End of sequence
[2024-09-25 11:59:32.747] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2465: Clean up the context of the stream work environment.
[2024-09-25 11:59:32.747] [info] [WASI-NN] llama.cpp: 
[2024-09-25 11:59:32.747] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  229559.05 ms
[2024-09-25 11:59:32.747] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =     200.65 ms /   109 runs   (    1.84 ms per token,   543.23 tokens per second)
[2024-09-25 11:59:32.747] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    6218.85 ms /   176 tokens (   35.33 ms per token,    28.30 tokens per second)
[2024-09-25 11:59:32.747] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    7668.12 ms /   107 runs   (   71.66 ms per token,    13.95 tokens per second)
[2024-09-25 11:59:32.747] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  237519.94 ms /   283 tokens
[2024-09-25 11:59:32.749] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2617: Cleanup done!
[2024-09-25 12:22:18.044] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:36458, local_addr: 0.0.0.0:8080
[2024-09-25 12:22:18.048] [info] rag_api_server in src/main.rs:495: method: POST, http_version: HTTP/1.1, content-length: 156
[2024-09-25 12:22:18.048] [info] rag_api_server in src/main.rs:496: endpoint: /v1/chat/completions
[2024-09-25 12:22:18.048] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-25 12:22:18.048] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-25 12:22:18.049] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-c3d87471-3dcf-4eb3-bdb4-4a0d731badb2
[2024-09-25 12:22:18.049] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-25 12:22:18.050] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Hello
[2024-09-25 12:22:18.050] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-25 12:22:18.050] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-25 12:22:18.050] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 12:22:18.050] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 12:22:18.051] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-25 12:22:18.051] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 12:22:18.051] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 12:22:18.051] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-25 12:22:18.051] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 8192
[2024-09-25 12:22:18.051] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-25 12:22:18.051] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-25 12:22:18.051] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 12:22:18.051] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-25 12:22:18.051] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 12:22:18.223] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
[2024-09-25 12:22:18.223] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
[2024-09-25 12:22:18.223] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-25 12:22:18.226] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-25 12:22:18.226] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-25 12:22:18.226] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 12:22:18.227] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-25 12:22:18.227] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 8192
[2024-09-25 12:22:18.227] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-25 12:22:18.227] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-25 12:22:18.227] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 12:22:18.227] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-25 12:22:18.227] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 12:22:18.272] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
[2024-09-25 12:22:18.272] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
[2024-09-25 12:22:18.272] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-25 12:22:18.273] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-25 12:22:18.273] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-25 12:22:18.273] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 12:22:18.315] [info] [WASI-NN] llama.cpp: 
[2024-09-25 12:22:18.315] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1603006.67 ms
[2024-09-25 12:22:18.315] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-25 12:22:18.315] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      24.37 ms /     3 tokens (    8.12 ms per token,   123.12 tokens per second)
[2024-09-25 12:22:18.315] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-25 12:22:18.315] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1603005.97 ms /     4 tokens
[2024-09-25 12:22:18.316] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named nomic-embed-text-v1.5.f16
[2024-09-25 12:22:18.316] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11261
[2024-09-25 12:22:18.332] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named nomic-embed-text-v1.5.f16.
[2024-09-25 12:22:18.333] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named nomic-embed-text-v1.5.f16
[2024-09-25 12:22:18.333] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-25 12:22:18.333] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 3, completion tokens: 0
[2024-09-25 12:22:18.333] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 3 prompt tokens, 0 comletion tokens
[2024-09-25 12:22:18.334] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-25 12:22:18.334] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-25 12:22:18.335] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 3, score_threshold: 0.5
[2024-09-25 12:22:18.335] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 12:22:18.335] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 12:22:18.335] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-25 12:22:18.489] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 3
[2024-09-25 12:22:18.489] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:388: point: 0, score: 0.53800714, source: "Conclusion:\n"
[2024-09-25 12:22:18.490] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:388: point: 1, score: 0.52540636, source: "Section 1: Introduction to Aeromodelling\n"
[2024-09-25 12:22:18.490] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:388: point: 2, score: 0.5004844, source: "Lift: Lift is the force that allows an aircraft to rise and stay airborne. It is created by the pressure difference between the upper and lower surfaces of the wings as air flows over them.\n"
[2024-09-25 12:22:18.490] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:87: Get the chat prompt template type from the chat model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 12:22:18.490] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:122: prompt_template: chatml
[2024-09-25 12:22:18.490] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:567: rag_policy: system-message
[2024-09-25 12:22:18.490] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:571: context:
"Conclusion:\n"

"Section 1: Introduction to Aeromodelling\n"

"Lift: Lift is the force that allows an aircraft to rise and stay airborne. It is created by the pressure difference between the upper and lower surfaces of the wings as air flows over them.\n"
[2024-09-25 12:22:18.490] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:575: Merge RAG context into system message.
[2024-09-25 12:22:18.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-25 12:22:18.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-25 12:22:18.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(true)
[2024-09-25 12:22:18.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:82: Process chat completion request in the stream mode.
[2024-09-25 12:22:18.491] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-25 12:22:18.491] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-25 12:22:18.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:104: user: chatcmpl-c3d87471-3dcf-4eb3-bdb4-4a0d731badb2
[2024-09-25 12:22:18.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:113: include_usage: false
[2024-09-25 12:22:18.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-25 12:22:18.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-25 12:22:18.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-25 12:22:18.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-25 12:22:18.493] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 12:22:18.493] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-25 12:22:18.493] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 4096
[2024-09-25 12:22:18.493] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 4096
[2024-09-25 12:22:18.493] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 12:22:18.493] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-25 12:22:18.493] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 12:22:18.690] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB
[2024-09-25 12:22:18.690] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
[2024-09-25 12:22:18.690] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB
[2024-09-25 12:22:18.691] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =  2406.00 MiB
[2024-09-25 12:22:18.691] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 846
[2024-09-25 12:22:18.691] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 12:22:18.694] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 12:22:18.694] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M
[2024-09-25 12:22:18.694] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 96
[2024-09-25 12:22:18.694] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 99, completion tokens: 0
[2024-09-25 12:22:18.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:124: prompt:
<|im_start|>system
You are a helpful assistant.
The following text is the context for the user question.\n----------------\n
"Conclusion:\n"

"Section 1: Introduction to Aeromodelling\n"

"Lift: Lift is the force that allows an aircraft to rise and stay airborne. It is created by the pressure difference between the upper and lower surfaces of the wings as air flows over them.\n"<|im_end|>
<|im_start|>user
Hello<|im_end|>
<|im_start|>assistant
[2024-09-25 12:22:18.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:125: available_completion_tokens: 820
[2024-09-25 12:22:18.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:126: tool_use: false
[2024-09-25 12:22:18.695] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-25 12:22:18.695] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-25 12:22:18.695] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-25 12:22:18.695] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Qwen1.5-1.8B-Chat-Q5_K_M.
[2024-09-25 12:22:18.695] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-25 12:22:18.695] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 4096
[2024-09-25 12:22:18.695] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 4096
[2024-09-25 12:22:18.695] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 12:22:18.695] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-25 12:22:18.695] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 12:22:18.799] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB
[2024-09-25 12:22:18.799] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
[2024-09-25 12:22:18.799] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB
[2024-09-25 12:22:18.801] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =  2406.00 MiB
[2024-09-25 12:22:18.801] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 846
[2024-09-25 12:22:18.801] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 12:22:18.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:193: End of the chat completion stream.
[2024-09-25 12:22:18.807] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:461: finish chat completions in stream mode
[2024-09-25 12:22:18.807] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-25 12:22:18.808] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-25 12:22:18.808] [info] rag_api_server in src/main.rs:517: response_body_size: 0
[2024-09-25 12:22:18.808] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-25 12:22:18.808] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-25 12:22:18.808] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 12:22:18.809] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-25 12:22:18.809] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 4096
[2024-09-25 12:22:18.809] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 4096
[2024-09-25 12:22:18.809] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-25 12:22:18.809] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-25 12:22:18.809] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-25 12:22:18.991] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB
[2024-09-25 12:22:18.991] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
[2024-09-25 12:22:18.991] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB
[2024-09-25 12:22:18.994] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =  2406.00 MiB
[2024-09-25 12:22:18.994] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 846
[2024-09-25 12:22:18.994] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-25 12:22:27.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 12:22:27.573] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 12:22:27.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 12:22:27.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 12:22:27.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 12:22:27.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 12:22:27.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-c3d87471-3dcf-4eb3-bdb4-4a0d731badb2","choices":[{"index":0,"delta":{"content":"Hello","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259747,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 12:22:27.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 12:22:27.757] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 12:22:27.757] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 12:22:27.757] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 12:22:27.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 12:22:27.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 12:22:27.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 12:22:27.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-c3d87471-3dcf-4eb3-bdb4-4a0d731badb2","choices":[{"index":0,"delta":{"content":"!","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259747,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 12:22:27.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 12:22:27.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 12:22:27.897] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 12:22:27.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 12:22:27.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 12:22:27.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 12:22:27.898] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 12:22:27.898] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-c3d87471-3dcf-4eb3-bdb4-4a0d731badb2","choices":[{"index":0,"delta":{"content":" How","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259747,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 12:22:27.898] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 12:22:28.036] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 12:22:28.036] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 12:22:28.037] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 12:22:28.037] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 12:22:28.037] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 12:22:28.037] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 12:22:28.037] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-c3d87471-3dcf-4eb3-bdb4-4a0d731badb2","choices":[{"index":0,"delta":{"content":" can","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259748,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 12:22:28.037] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 12:22:28.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 12:22:28.181] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 12:22:28.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 12:22:28.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 12:22:28.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 12:22:28.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 12:22:28.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-c3d87471-3dcf-4eb3-bdb4-4a0d731badb2","choices":[{"index":0,"delta":{"content":" I","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259748,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 12:22:28.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 12:22:28.322] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 12:22:28.322] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 12:22:28.322] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 12:22:28.322] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 12:22:28.322] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 12:22:28.323] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 12:22:28.323] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-c3d87471-3dcf-4eb3-bdb4-4a0d731badb2","choices":[{"index":0,"delta":{"content":" help","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259748,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 12:22:28.323] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 12:22:28.447] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 12:22:28.447] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 12:22:28.447] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 12:22:28.447] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 12:22:28.447] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 12:22:28.447] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 12:22:28.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-c3d87471-3dcf-4eb3-bdb4-4a0d731badb2","choices":[{"index":0,"delta":{"content":" you","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259748,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 12:22:28.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 12:22:28.607] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 12:22:28.607] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 12:22:28.607] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 12:22:28.607] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 12:22:28.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 12:22:28.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 12:22:28.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-c3d87471-3dcf-4eb3-bdb4-4a0d731badb2","choices":[{"index":0,"delta":{"content":" today","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259748,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 12:22:28.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 12:22:28.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 12:22:28.787] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 12:22:28.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 12:22:28.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 12:22:28.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 12:22:28.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 12:22:28.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-c3d87471-3dcf-4eb3-bdb4-4a0d731badb2","choices":[{"index":0,"delta":{"content":"?","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259748,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 12:22:28.789] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 12:22:28.949] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 12:22:28.949] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 12:22:28.949] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 12:22:28.949] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 12:22:28.949] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 12:22:28.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 12:22:28.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-c3d87471-3dcf-4eb3-bdb4-4a0d731badb2","choices":[{"index":0,"delta":{"content":" If","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259748,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 12:22:28.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 12:22:29.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 12:22:29.088] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 12:22:29.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 12:22:29.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 12:22:29.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 12:22:29.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 12:22:29.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-c3d87471-3dcf-4eb3-bdb4-4a0d731badb2","choices":[{"index":0,"delta":{"content":" you","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259749,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 12:22:29.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 12:22:29.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 12:22:29.241] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 12:22:29.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 12:22:29.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 12:22:29.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 12:22:29.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 12:22:29.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-c3d87471-3dcf-4eb3-bdb4-4a0d731badb2","choices":[{"index":0,"delta":{"content":" have","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259749,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 12:22:29.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 12:22:29.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 12:22:29.378] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 12:22:29.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 12:22:29.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 12:22:29.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 12:22:29.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 12:22:29.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-c3d87471-3dcf-4eb3-bdb4-4a0d731badb2","choices":[{"index":0,"delta":{"content":" any","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259749,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 12:22:29.379] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 12:22:29.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 12:22:29.530] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 12:22:29.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 12:22:29.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 12:22:29.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 12:22:29.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 12:22:29.531] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-c3d87471-3dcf-4eb3-bdb4-4a0d731badb2","choices":[{"index":0,"delta":{"content":" specific","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259749,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 12:22:29.531] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 12:22:29.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 12:22:29.672] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 12:22:29.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 12:22:29.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 12:22:29.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 12:22:29.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 12:22:29.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-c3d87471-3dcf-4eb3-bdb4-4a0d731badb2","choices":[{"index":0,"delta":{"content":" questions","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259749,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 12:22:29.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-25 12:22:29.813] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-25 12:22:29.813] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Qwen1.5-1.8B-Chat-Q5_K_M in the stream mode.
[2024-09-25 12:22:29.813] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-25 12:22:29.813] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-25 12:22:29.813] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-25 12:22:29.814] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-25 12:22:29.814] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-c3d87471-3dcf-4eb3-bdb4-4a0d731badb2","choices":[{"index":0,"delta":{"content":" or","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727259749,"model":"Qwen1.5-1.8B-Chat-Q5_K_M","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-25 12:22:29.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2465: Clean up the context of the stream work environment.
[2024-09-25 12:22:29.815] [info] [WASI-NN] llama.cpp: 
[2024-09-25 12:22:29.815] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1612193.17 ms
[2024-09-25 12:22:29.815] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =      57.26 ms /    16 runs   (    3.58 ms per token,   279.44 tokens per second)
[2024-09-25 12:22:29.815] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    8425.85 ms /    99 tokens (   85.11 ms per token,    11.75 tokens per second)
[2024-09-25 12:22:29.815] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    2170.19 ms /    15 runs   (  144.68 ms per token,     6.91 tokens per second)
[2024-09-25 12:22:29.815] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1614587.94 ms /   114 tokens
[2024-09-25 12:22:29.818] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2617: Cleanup done!
